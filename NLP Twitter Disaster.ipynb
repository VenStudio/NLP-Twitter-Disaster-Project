{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NLP Twitter Disaster Classifier Project**\n",
    "This project aims to classify tweets based on their content on whether they are commenting on real disasters or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **1. Importing data**\n",
    "Data set from [Keggle](https://www.kaggle.com/competitions/nlp-getting-started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('..\\\\Datasets\\\\Twitter Disaster\\\\train.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **2. Preprocessing data**\n",
    "**This includes:**\n",
    "- Removing unnecessary text.\n",
    "- Converting the text to lowercase.\n",
    "- Tokenizing the text.\n",
    "- Removing stopwords.\n",
    "- Applying Lemmatization.\n",
    "- Extracting important info (such as hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>#earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>#wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>#alaska #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant crane holding bridge collapse nearby...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>aria ahrary thetawniest control wild fire cali...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>utc km volcano hawaii</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>police investigating e bike collided car littl...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>latest home razed northern california wildfire...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "                                           cleaned_text            hashtags  \n",
       "0            deed reason earthquake may allah forgive u         #earthquake  \n",
       "1                 forest fire near la ronge sask canada                      \n",
       "2     resident asked shelter place notified officer ...                      \n",
       "3     people receive wildfire evacuation order calif...          #wildfires  \n",
       "4     got sent photo ruby alaska smoke wildfire pour...  #alaska #wildfires  \n",
       "...                                                 ...                 ...  \n",
       "7608  two giant crane holding bridge collapse nearby...                      \n",
       "7609  aria ahrary thetawniest control wild fire cali...                      \n",
       "7610                              utc km volcano hawaii                      \n",
       "7611  police investigating e bike collided car littl...                      \n",
       "7612  latest home razed northern california wildfire...                      \n",
       "\n",
       "[7613 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Removes URLs, HTML tags, Hashtags, then converts text to lowercase, and tokenizes and lemmatizes it.\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+', ' ', text)\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Processing Hashtags specifically\n",
    "def preprocess_hashtags(text):\n",
    "    text = text.lower()\n",
    "    text = re.findall(r'#\\w[\\w_]*', text)\n",
    "    return ' '.join(text)\n",
    "\n",
    "# Apply preprocessing to the tweet text\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "# Extract hashtags to a seperate column\n",
    "df['hashtags'] = df['text'].apply(preprocess_hashtags)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **3. Feature Extraction**\n",
    "using TF-IDF, vectorize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7613x16724 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 70105 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "y = df['target']\n",
    "\n",
    "# Vectorize and process hashtags\n",
    "X_hashtags = tfidf_vectorizer.fit_transform(df['hashtags'])\n",
    "hashtag_set = ['#disaster', '#earthquake', '#flood', '#fire'] \n",
    "# Add binary features for each hashtag in the hashtag_set\n",
    "for hashtag in hashtag_set:\n",
    "    df[hashtag] = df['hashtags'].apply(lambda x: int(hashtag in x))\n",
    "\n",
    "\n",
    "# Merge cleaned_text and hashtags into a single column\n",
    "X = hstack([X, X_hashtags])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **4-5.Model Training and Evaluation**\n",
    "train multiple models and select the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (6090, 16724)\n",
      "Test data shape: (1523, 16724)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=26)\n",
    "\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "print(f'Test data shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Logistic Regression...\n",
      "Logistic Regression Performance:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84       893\n",
      "           1       0.80      0.70      0.75       630\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.80      0.79      0.79      1523\n",
      "weighted avg       0.80      0.80      0.80      1523\n",
      "\n",
      "Mean Cross-Validation F1 Score: 0.809583074682472\n",
      "Confusion Matrix:\n",
      " [[781 112]\n",
      " [187 443]]\n",
      "\n",
      "\n",
      "Evaluating SVM...\n",
      "SVM Performance:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.85       893\n",
      "           1       0.85      0.64      0.73       630\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.82      0.78      0.79      1523\n",
      "weighted avg       0.81      0.80      0.80      1523\n",
      "\n",
      "Mean Cross-Validation F1 Score: 0.814342359423837\n",
      "Confusion Matrix:\n",
      " [[823  70]\n",
      " [227 403]]\n",
      "\n",
      "\n",
      "Evaluating XGBoost...\n",
      "XGBoost Performance:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       893\n",
      "           1       0.78      0.66      0.72       630\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.78      0.77      0.77      1523\n",
      "weighted avg       0.78      0.78      0.78      1523\n",
      "\n",
      "Mean Cross-Validation F1 Score: 0.7767249023126622\n",
      "Confusion Matrix:\n",
      " [[776 117]\n",
      " [212 418]]\n",
      "\n",
      "\n",
      "Evaluating Random Forest...\n",
      "Random Forest Performance:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.83       893\n",
      "           1       0.81      0.66      0.72       630\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.80      0.77      0.78      1523\n",
      "weighted avg       0.79      0.79      0.79      1523\n",
      "\n",
      "Mean Cross-Validation F1 Score: 0.7967066124180169\n",
      "Confusion Matrix:\n",
      " [[793 100]\n",
      " [216 414]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Resample data to handle imbalance\n",
    "smote = SMOTE(random_state=26)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# dictionary for all the models used\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=1000),\n",
    "    'SVM': SVC(class_weight='balanced', probability=True),\n",
    "    'XGBoost': xgb.XGBClassifier(scale_pos_weight=(np.sum(y_train == 0)/np.sum(y_train == 1)), eval_metric='mlogloss'),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "}\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=26)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='f1')\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "    evaluation_results[model_name] = {\n",
    "        'Classification Report': classification_rep,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Mean Cross-Validation F1 Score': mean_cv_score,\n",
    "    }\n",
    "    \n",
    "    print(f'{model_name} Performance:')\n",
    "    print('Classification Report:\\n', classification_rep)\n",
    "    print('Mean Cross-Validation F1 Score:', mean_cv_score)\n",
    "    print('Confusion Matrix:\\n', conf_matrix)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **6.  Interpretation and Application**\n",
    "using metrics, find the best possible model to be used on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "The best model is: SVM\n",
      "======================================\n",
      "Metrics:\n",
      "---------------------\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.85       893\n",
      "           1       0.85      0.64      0.73       630\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.82      0.78      0.79      1523\n",
      "weighted avg       0.81      0.80      0.80      1523\n",
      "\n",
      "Confusion Matrix:\n",
      " [[823  70]\n",
      " [227 403]]\n",
      "Mean Cross-Validation F1 Score: 0.814342359423837\n"
     ]
    }
   ],
   "source": [
    "# Determine the best model based on Mean Cross-Validation F1 Score\n",
    "best_model_name = max(evaluation_results, key=lambda k: evaluation_results[k]['Mean Cross-Validation F1 Score'])\n",
    "best_model_metrics = evaluation_results[best_model_name]\n",
    "\n",
    "# Print the model's info\n",
    "print(f'======================================')\n",
    "print(f\"The best model is: {best_model_name}\")\n",
    "print(f'======================================')\n",
    "print(f'Metrics:\\n---------------------')\n",
    "print('Classification Report:', best_model_metrics['Classification Report'])\n",
    "print('Confusion Matrix:\\n', best_model_metrics['Confusion Matrix'])\n",
    "print('Mean Cross-Validation F1 Score:', best_model_metrics['Mean Cross-Validation F1 Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How Can We Use This Model Efficiently?**\n",
    "\n",
    "#### **Real-Time Monitoring**\n",
    "\n",
    "We can **connect** the model to social media to keep an eye on **disaster-related tweets**. This way, it can automatically **alert authorities** about new situations, helping them respond quickly and coordinate efforts.\n",
    "\n",
    "#### **Spotting Key Information**\n",
    "\n",
    "The model helps **pick out** and **highlight** crucial info from social media, making sure that **important updates** get to the right people in time. This helps affected communities stay informed.\n",
    "\n",
    "#### **Guiding Resource Use**\n",
    "\n",
    "It also looks at **tweet trends** to help decide where **resources** should go. By showing how **serious** and **widespread** a disaster is, the model supports **smarter planning** and quicker responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
